{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa3232b3",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1aa0901",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class MLP(nn.module):\n",
    "    def __init__(self,input_size,hidden_size,num_classes):\n",
    "        super(MLP,self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = x.view(-1, self.input_size)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        '''\n",
    "        或者这样写\n",
    "        x = self.fc1(x)\n",
    "        x = torch.relu(x)\n",
    "        '''\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "# 1. 创建一个MLP实例\n",
    "input_size = 576 # 假设每个输入有784个特征（例如，24x24像素的图像）\n",
    "hidden_size = 512 # 第一个隐藏层的神经元数量\n",
    "num_classes = 10 # 输出类别数量\n",
    "mlp = MLP(input_size, hidden_size, num_classes)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(mlp.parameters(), lr=1e-4)\n",
    "\n",
    "x_train = torch.randn(64, input_size) \n",
    "y_train = torch.randn(64, num_classes)\n",
    "\n",
    "for epoch in range(100):\n",
    "    # 进行100个训练周期\n",
    "    # 前向传播 \n",
    "    outputs = mlp(x_train) \n",
    "    loss = criterion(outputs, y_train)\n",
    "\n",
    "    # 反向传播和优化 \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward() \n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/100], Loss: {loss.item():.4f}')\n",
    "\n",
    "with torch.no_grad():\n",
    "    sample_data = torch.randn(1, input_size) \n",
    "    predictions = mlp(sample_data) \n",
    "    print(predictions)\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b25370",
   "metadata": {},
   "source": [
    "# Self-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a101bdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class SelfAttention(nn.module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.q_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.k_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.v_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        q = self.q_proj(x)\n",
    "        k = self.k_proj(x)\n",
    "        v = self.v_proj(x)\n",
    "        attn_weight = torch.matmul(q, k.transpose(-2, -1))\n",
    "        attn_weight = torch.softmax(attn_weight/math.sqrt(self.hidden_dim), dim=-1)\n",
    "        output = torch.matmul(attn_weight, v)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f162056",
   "metadata": {},
   "source": [
    "# MHA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a59626",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MHA(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_heads,attention_dropout=0.1, output_dropout=0.1):\n",
    "        super(MHA, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_dim / num_heads\n",
    "        self.q_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.k_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.v_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.out_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.attn_dropout = nn.Dropout(attention_dropout)\n",
    "        self.output_dropout = nn.Dropout(output_dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        # shape 变成 （batch_size, num_head, seq_len, head_dim）\n",
    "        Q = self.q_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim).permute(#另一种写法\n",
    "            0, 2, 1, 3\n",
    "        )\n",
    "        K = self.k_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = self.v_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        attn_weight = torch.softmax(Q @ K.transpose(-2, -1)/math.sqrt(self.head_dim),dim = -1)\n",
    "        attn_weight = self.dropout(attn_weight)\n",
    "        output = attn_weight @ V\n",
    "        # contiguous重新分配内存，使张量在内存中是连续的，view函数要求输入的张量在内存中是连续的\n",
    "        output = output.transpose(1,2).contiguous().view(batch_size, seq_len, self.hidden_dim)\n",
    "        output = self.out_proj(output)\n",
    "        output = self.output_dropout(output)\n",
    "        \n",
    "        return output\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35492a0e",
   "metadata": {},
   "source": [
    "# 交叉熵+Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8ff0ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits:\n",
      " tensor([[ 0.5000,  2.0000, -1.0000],\n",
      "        [ 3.0000,  0.1000,  0.1000],\n",
      "        [-2.0000,  0.5000,  1.5000]])\n",
      "\n",
      "Probabilities (after manual_softmax):\n",
      " tensor([[0.1753, 0.7856, 0.0391],\n",
      "        [0.9009, 0.0496, 0.0496],\n",
      "        [0.0216, 0.2631, 0.7153]])\n",
      "\n",
      "Manual Softmax + Manual Cross Entropy Loss: 0.2269\n",
      "PyTorch nn.CrossEntropyLoss (takes logits): 0.2269\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def mannul_softmax(x):\n",
    "    \"\"\"\n",
    "    手动实现softmax函数\n",
    "    :param x: 输入数组\n",
    "    :return: softmax输出\n",
    "    \"\"\"\n",
    "    # 防止溢出，减去最大值\n",
    "    # torch.max(x, axis=1, keepdims=True)返回一个元组，第一个元素是最大值，第二个元素是最大值的索引\n",
    "    # 使用values属性获取最大值,indices属性获取索引\n",
    "    max_val = torch.max(x, axis=1, keepdims=True).values\n",
    "    e_x = torch.exp(x - max_val)\n",
    "    return e_x / torch.sum(e_x, dim=1, keepdims=True)\n",
    "\n",
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    计算交叉熵损失\n",
    "    :param y_true: 真实标签，one-hot编码\n",
    "    :param y_pred: 预测值，softmax输出\n",
    "    :return: 交叉熵损失\n",
    "    \"\"\"\n",
    "    # 防止log(0)的情况\n",
    "    epsilon = 1e-15\n",
    "    #数组 a 中所有小于 a_min 的值会被替换为 a_min。\n",
    "    #数组 a 中所有大于 a_max 的值会被替换为 a_max。\n",
    "    # y_pred = mannul_softmax(y_pred)\n",
    "    y_pred = torch.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    \n",
    "    # 计算交叉熵损失\n",
    "    loss = -torch.sum(y_true * torch.log(y_pred), axis=1)\n",
    "    \n",
    "    return torch.mean(loss)\n",
    "\n",
    "\n",
    "# 示例\n",
    "# 1. 真实标签 (one-hot 编码)\n",
    "y_true = torch.tensor([[0, 1, 0],\n",
    "                       [1, 0, 0],\n",
    "                       [0, 0, 1]], dtype=torch.float32) # 使用 float 类型以匹配计算\n",
    "\n",
    "# 2. 模拟模型的原始输出 (logits - 未经 Softmax)\n",
    "logits = torch.tensor([[0.5, 2.0, -1.0],  # 第一个样本，类别1得分最高\n",
    "                       [3.0, 0.1, 0.1],  # 第二个样本，类别0得分最高\n",
    "                       [-2.0, 0.5, 1.5]], # 第三个样本，类别2得分最高\n",
    "                      dtype=torch.float32)\n",
    "\n",
    "# 3. 使用手动实现的 Softmax 将 logits 转换为概率\n",
    "y_pred_probs = mannul_softmax(logits)\n",
    "print(\"Logits:\\n\", logits)\n",
    "print(\"\\nProbabilities (after manual_softmax):\\n\", y_pred_probs)\n",
    "\n",
    "# 4. 使用手动实现的交叉熵损失函数计算损失\n",
    "manual_loss = cross_entropy_loss(y_true, y_pred_probs)\n",
    "print(f\"\\nManual Softmax + Manual Cross Entropy Loss: {manual_loss.item():.4f}\")\n",
    "\n",
    "# 5. (可选) 使用 PyTorch 内置函数进行验证\n",
    "# 注意: nn.CrossEntropyLoss 结合了 Softmax 和 NLLLoss，它期望输入原始 logits 和类别索引\n",
    "criterion_pytorch = nn.CrossEntropyLoss()\n",
    "# 将 one-hot 真实标签转换为类别索引\n",
    "y_true_indices = torch.argmax(y_true, dim=1)\n",
    "pytorch_loss = criterion_pytorch(logits, y_true_indices)\n",
    "print(f\"PyTorch nn.CrossEntropyLoss (takes logits): {pytorch_loss.item():.4f}\")\n",
    "\n",
    "# 比较两个损失值，它们应该非常接近（可能因数值精度略有差异）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42926e1c",
   "metadata": {},
   "source": [
    "# KL散度\n",
    "**真实分布 (p)​**​  \n",
    "  例：`[1, 0, 0]`（样本明确属于第一类）  \n",
    "  信息熵（最小平均编码长度）：\n",
    "  $$\n",
    "  H(p) = -\\sum_{i=1}^C p(x_i)\\log p(x_i)\n",
    "  $$\n",
    "\n",
    "**拟合分布 (q)​**​  \n",
    "  例：`[0.7, 0.2, 0.1]`（模型预测概率）  \n",
    "  交叉熵（实际编码长度期望）：\n",
    "  $$\n",
    "  H(p,q) = -\\sum_{i=1}^C p(x_i)\\log q(x_i)\n",
    "  $$\n",
    "\n",
    "**KL散度**​  \n",
    "  衡量分布差异（吉布斯不等式保证 $H(p,q) \\geq H(p)$）：\n",
    "  $$\n",
    "  D_{KL}(p \\| q) = H(p,q) - H(p) = \\sum_{i=1}^C p(x_i)\\log \\frac{p(x_i)}{q(x_i)}\n",
    "  $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23237b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1 - KL Divergence: 0.2269\n",
      "Sample 2 - KL Divergence: 0.0258\n"
     ]
    }
   ],
   "source": [
    "def kl_divergence(y_true, y_pred, eps=1e-15):\n",
    "    \"\"\"\n",
    "    计算 KL 散度，即 D_KL(p || q)\n",
    "    \n",
    "    参数:\n",
    "        p: torch.Tensor, 真实概率分布 (每行代表一个样本的概率分布)\n",
    "        q: torch.Tensor, 预测概率分布 (每行代表一个样本的概率分布)\n",
    "        eps: float, 防止 log(0) 的值\n",
    "        \n",
    "    返回:\n",
    "        平均 KL 散度\n",
    "    \"\"\"\n",
    "    y_true = torch.clamp(y_true, eps, 1)\n",
    "    y_pred = torch.clamp(y_pred, eps, 1)\n",
    "    # 对每个样本，计算 sum(p * (log(p) - log(q)))\n",
    "    kl = torch.sum(y_true * (torch.log(y_true) - torch.log(y_pred)), dim=1)\n",
    "    return torch.mean(kl)\n",
    "# 示例 1：使用已有的 y_true 和 y_pred_probs\n",
    "# 计算结果中可以得知，在分类任务中，交叉熵损失等于KL散度\n",
    "kl1 = kl_divergence(y_true, y_pred_probs)\n",
    "print(f\"Sample 1 - KL Divergence: {kl1.item():.4f}\")\n",
    "\n",
    "# 示例 2：自定义的概率分布\n",
    "p2 = torch.tensor([[0.4, 0.4, 0.2]], dtype=torch.float32)\n",
    "q2 = torch.tensor([[0.3, 0.5, 0.2]], dtype=torch.float32)\n",
    "kl2 = kl_divergence(p2, q2)\n",
    "print(f\"Sample 2 - KL Divergence: {kl2.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fdcc10",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
