{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa3232b3",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1aa0901",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class MLP(nn.module):\n",
    "    def __init__(self,input_size,hidden_size,num_classes):\n",
    "        super(MLP,self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = x.view(-1, self.input_size)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        '''\n",
    "        或者这样写\n",
    "        x = self.fc1(x)\n",
    "        x = torch.relu(x)\n",
    "        '''\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "# 1. 创建一个MLP实例\n",
    "input_size = 576 # 假设每个输入有784个特征（例如，24x24像素的图像）\n",
    "hidden_size = 512 # 第一个隐藏层的神经元数量\n",
    "num_classes = 10 # 输出类别数量\n",
    "mlp = MLP(input_size, hidden_size, num_classes)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(mlp.parameters(), lr=1e-4)\n",
    "\n",
    "x_train = torch.randn(64, input_size) \n",
    "y_train = torch.randn(64, num_classes)\n",
    "\n",
    "for epoch in range(100):\n",
    "    # 进行100个训练周期\n",
    "    # 前向传播 \n",
    "    outputs = mlp(x_train) \n",
    "    loss = criterion(outputs, y_train)\n",
    "\n",
    "    # 反向传播和优化 \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward() \n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/100], Loss: {loss.item():.4f}')\n",
    "\n",
    "with torch.no_grad():\n",
    "    sample_data = torch.randn(1, input_size) \n",
    "    predictions = mlp(sample_data) \n",
    "    print(predictions)\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b25370",
   "metadata": {},
   "source": [
    "# Self-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a101bdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class SelfAttention(nn.module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.q_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.k_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.v_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        q = self.q_proj(x)\n",
    "        k = self.k_proj(x)\n",
    "        v = self.v_proj(x)\n",
    "        attn_weight = torch.matmul(q, k.transpose(-2, -1))\n",
    "        attn_weight = torch.softmax(attn_weight/math.sqrt(self.hidden_dim), dim=-1)\n",
    "        output = torch.matmul(attn_weight, v)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f162056",
   "metadata": {},
   "source": [
    "# MHA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a59626",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MHA(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_heads,attention_dropout=0.1, output_dropout=0.1):\n",
    "        super(MHA, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_dim / num_heads\n",
    "        self.q_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.k_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.v_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.out_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.attn_dropout = nn.Dropout(attention_dropout)\n",
    "        self.output_dropout = nn.Dropout(output_dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        # shape 变成 （batch_size, num_head, seq_len, head_dim）\n",
    "        Q = self.q_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim).permute(#另一种写法\n",
    "            0, 2, 1, 3\n",
    "        )\n",
    "        K = self.k_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = self.v_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        attn_weight = torch.softmax(Q @ K.transpose(-2, -1)/math.sqrt(self.head_dim),dim = -1)\n",
    "        attn_weight = self.dropout(attn_weight)\n",
    "        output = attn_weight @ V\n",
    "        # contiguous重新分配内存，使张量在内存中是连续的，view函数要求输入的张量在内存中是连续的\n",
    "        output = output.transpose(1,2).contiguous().view(batch_size, seq_len, self.hidden_dim)\n",
    "        output = self.out_proj(output)\n",
    "        output = self.output_dropout(output)\n",
    "        \n",
    "        return output\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35492a0e",
   "metadata": {},
   "source": [
    "# 交叉熵+Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8ff0ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits:\n",
      " tensor([[ 0.5000,  2.0000, -1.0000],\n",
      "        [ 3.0000,  0.1000,  0.1000],\n",
      "        [-2.0000,  0.5000,  1.5000]])\n",
      "\n",
      "Probabilities (after manual_softmax):\n",
      " tensor([[0.1753, 0.7856, 0.0391],\n",
      "        [0.9009, 0.0496, 0.0496],\n",
      "        [0.0216, 0.2631, 0.7153]])\n",
      "\n",
      "Manual Softmax + Manual Cross Entropy Loss: 0.2269\n",
      "PyTorch nn.CrossEntropyLoss (takes logits): 0.2269\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def mannul_softmax(x):\n",
    "    \"\"\"\n",
    "    手动实现softmax函数\n",
    "    :param x: 输入数组\n",
    "    :return: softmax输出\n",
    "    \"\"\"\n",
    "    # 防止溢出，减去最大值\n",
    "    # torch.max(x, axis=1, keepdims=True)返回一个元组，第一个元素是最大值，第二个元素是最大值的索引\n",
    "    # 使用values属性获取最大值,indices属性获取索引\n",
    "    max_val = torch.max(x, axis=1, keepdims=True).values\n",
    "    e_x = torch.exp(x - max_val)\n",
    "    return e_x / torch.sum(e_x, dim=1, keepdims=True)\n",
    "\n",
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    计算交叉熵损失\n",
    "    :param y_true: 真实标签，one-hot编码\n",
    "    :param y_pred: 预测值，softmax输出\n",
    "    :return: 交叉熵损失\n",
    "    \"\"\"\n",
    "    # 防止log(0)的情况\n",
    "    epsilon = 1e-15\n",
    "    #数组 a 中所有小于 a_min 的值会被替换为 a_min。\n",
    "    #数组 a 中所有大于 a_max 的值会被替换为 a_max。\n",
    "    # y_pred = mannul_softmax(y_pred)\n",
    "    y_pred = torch.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    \n",
    "    # 计算交叉熵损失\n",
    "    loss = -torch.sum(y_true * torch.log(y_pred), axis=1)\n",
    "    \n",
    "    return torch.mean(loss)\n",
    "\n",
    "\n",
    "# 示例\n",
    "# 1. 真实标签 (one-hot 编码)\n",
    "y_true = torch.tensor([[0, 1, 0],\n",
    "                       [1, 0, 0],\n",
    "                       [0, 0, 1]], dtype=torch.float32) # 使用 float 类型以匹配计算\n",
    "\n",
    "# 2. 模拟模型的原始输出 (logits - 未经 Softmax)\n",
    "logits = torch.tensor([[0.5, 2.0, -1.0],  # 第一个样本，类别1得分最高\n",
    "                       [3.0, 0.1, 0.1],  # 第二个样本，类别0得分最高\n",
    "                       [-2.0, 0.5, 1.5]], # 第三个样本，类别2得分最高\n",
    "                      dtype=torch.float32)\n",
    "\n",
    "# 3. 使用手动实现的 Softmax 将 logits 转换为概率\n",
    "y_pred_probs = mannul_softmax(logits)\n",
    "print(\"Logits:\\n\", logits)\n",
    "print(\"\\nProbabilities (after manual_softmax):\\n\", y_pred_probs)\n",
    "\n",
    "# 4. 使用手动实现的交叉熵损失函数计算损失\n",
    "manual_loss = cross_entropy_loss(y_true, y_pred_probs)\n",
    "print(f\"\\nManual Softmax + Manual Cross Entropy Loss: {manual_loss.item():.4f}\")\n",
    "\n",
    "# 5. (可选) 使用 PyTorch 内置函数进行验证\n",
    "# 注意: nn.CrossEntropyLoss 结合了 Softmax 和 NLLLoss，它期望输入原始 logits 和类别索引\n",
    "criterion_pytorch = nn.CrossEntropyLoss()\n",
    "# 将 one-hot 真实标签转换为类别索引\n",
    "y_true_indices = torch.argmax(y_true, dim=1)\n",
    "pytorch_loss = criterion_pytorch(logits, y_true_indices)\n",
    "print(f\"PyTorch nn.CrossEntropyLoss (takes logits): {pytorch_loss.item():.4f}\")\n",
    "\n",
    "# 比较两个损失值，它们应该非常接近（可能因数值精度略有差异）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42926e1c",
   "metadata": {},
   "source": [
    "# KL散度\n",
    "**真实分布 (p)​**​  \n",
    "  例：`[1, 0, 0]`（样本明确属于第一类）  \n",
    "  信息熵（最小平均编码长度）：\n",
    "  $$\n",
    "  H(p) = -\\sum_{i=1}^C p(x_i)\\log p(x_i)\n",
    "  $$\n",
    "\n",
    "**拟合分布 (q)​**​  \n",
    "  例：`[0.7, 0.2, 0.1]`（模型预测概率）  \n",
    "  交叉熵（实际编码长度期望）：\n",
    "  $$\n",
    "  H(p,q) = -\\sum_{i=1}^C p(x_i)\\log q(x_i)\n",
    "  $$\n",
    "\n",
    "**KL散度**​  \n",
    "  衡量分布差异（吉布斯不等式保证 $H(p,q) \\geq H(p)$）：\n",
    "  $$\n",
    "  D_{KL}(p \\| q) = H(p,q) - H(p) = \\sum_{i=1}^C p(x_i)\\log \\frac{p(x_i)}{q(x_i)}\n",
    "  $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23237b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1 - KL Divergence: 0.2269\n",
      "Sample 2 - KL Divergence: 0.0258\n"
     ]
    }
   ],
   "source": [
    "def kl_divergence(y_true, y_pred, eps=1e-15):\n",
    "    \"\"\"\n",
    "    计算 KL 散度，即 D_KL(p || q)\n",
    "    \n",
    "    参数:\n",
    "        p: torch.Tensor, 真实概率分布 (每行代表一个样本的概率分布)\n",
    "        q: torch.Tensor, 预测概率分布 (每行代表一个样本的概率分布)\n",
    "        eps: float, 防止 log(0) 的值\n",
    "        \n",
    "    返回:\n",
    "        平均 KL 散度\n",
    "    \"\"\"\n",
    "    y_true = torch.clamp(y_true, eps, 1)\n",
    "    y_pred = torch.clamp(y_pred, eps, 1)\n",
    "    # 对每个样本，计算 sum(p * (log(p) - log(q)))\n",
    "    kl = torch.sum(y_true * (torch.log(y_true) - torch.log(y_pred)), dim=1)\n",
    "    return torch.mean(kl)\n",
    "# 示例 1：使用已有的 y_true 和 y_pred_probs\n",
    "# 计算结果中可以得知，在分类任务中，交叉熵损失等于KL散度\n",
    "kl1 = kl_divergence(y_true, y_pred_probs)\n",
    "print(f\"Sample 1 - KL Divergence: {kl1.item():.4f}\")\n",
    "\n",
    "# 示例 2：自定义的概率分布\n",
    "p2 = torch.tensor([[0.4, 0.4, 0.2]], dtype=torch.float32)\n",
    "q2 = torch.tensor([[0.3, 0.5, 0.2]], dtype=torch.float32)\n",
    "kl2 = kl_divergence(p2, q2)\n",
    "print(f\"Sample 2 - KL Divergence: {kl2.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fdcc10",
   "metadata": {},
   "source": [
    "# LN and BN\n",
    "Layer Normalization (LN) 和 Batch Normalization (BN) 的主要区别在于归一化的维度：  \n",
    "- BN 在计算时使用同一 mini-batch 内所有样本的统计数据（均值和方差）进行归一化，这使其对 batch size 依赖较大，并且在 NLP 模型中难以处理变长序列；  \n",
    "- LN 则对单个样本内部的所有特征进行归一化，不依赖于 batch 内其他样本，因此在处理序列数据（如大语言模型）时更加稳定。  \n",
    "\n",
    "大语言模型（例如 Transformer）通常使用 Layer Normalization，因为它能更好地适应小批量或甚至单样本训练，并且在长序列时能够保持稳定的训练过程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830fd20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    手动实现 Layer Normalization\n",
    "    \"\"\"\n",
    "    def __init__(self, normalized_shape, eps=1e-5):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        # normalized_shape 可以是一个整数（表示最后一个维度的大小）\n",
    "        # 或一个 torch.Size 对象（表示需要归一化的维度）\n",
    "        if isinstance(normalized_shape, int):\n",
    "            normalized_shape = (normalized_shape,)\n",
    "        self.normalized_shape = torch.Size(normalized_shape)\n",
    "        self.eps = eps\n",
    "        # 创建可学习的缩放参数 gamma 和平移参数 beta\n",
    "        # 使用 nn.Parameter 将它们注册为模型参数\n",
    "        self.gamma = nn.Parameter(torch.ones(self.normalized_shape))\n",
    "        self.beta = nn.Parameter(torch.zeros(self.normalized_shape))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 计算需要归一化的维度的均值和方差\n",
    "        # dims = tuple(range(x.dim() - len(self.normalized_shape), x.dim()))\n",
    "        # PyTorch LayerNorm 默认对最后一个维度(或多个维度)进行归一化\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, unbiased=False, keepdim=True) # 使用有偏方差\n",
    "\n",
    "        # 归一化\n",
    "        x_normalized = (x - mean) / torch.sqrt(var + self.eps)\n",
    "\n",
    "        # 应用可学习的缩放和平移\n",
    "        out = self.gamma * x_normalized + self.beta\n",
    "        return out\n",
    "\n",
    "class BatchNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    手动实现 Batch Normalization (针对 1D 或 2D 输入，特征在最后一维)\n",
    "    \"\"\"\n",
    "    def __init__(self, num_features, eps=1e-5, momentum=0.1):\n",
    "        super(BatchNorm, self).__init__()\n",
    "        self.num_features = num_features\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        # 可学习的参数 gamma 和 beta\n",
    "        self.gamma = nn.Parameter(torch.ones(num_features))\n",
    "        self.beta = nn.Parameter(torch.zeros(num_features))\n",
    "        # 运行时的均值和方差，用于推理阶段\n",
    "        # 使用 register_buffer 注册，它们是模型状态的一部分，但不参与梯度计算\n",
    "        self.register_buffer('running_mean', torch.zeros(num_features))\n",
    "        self.register_buffer('running_var', torch.ones(num_features))\n",
    "        # 设置为训练模式\n",
    "        self.train()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x 的形状通常是 (batch_size, num_features) 或 (batch_size, seq_len, num_features)\n",
    "        # BN 通常在特征维度上操作\n",
    "        if x.dim() > 2: # 例如 (batch, seq_len, features) -> (batch * seq_len, features)\n",
    "             x_reshaped = x.contiguous().view(-1, self.num_features)\n",
    "        else: # (batch, features)\n",
    "             x_reshaped = x\n",
    "\n",
    "        if self.training:\n",
    "            # 计算当前 mini-batch 的均值和方差 (在 batch 维度上)\n",
    "            batch_mean = x_reshaped.mean(dim=0)\n",
    "            batch_var = x_reshaped.var(dim=0, unbiased=False) # 使用有偏方差\n",
    "\n",
    "            # 更新运行时的均值和方差\n",
    "            self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * batch_mean\n",
    "            self.running_var = (1 - self.momentum) * self.running_var + self.momentum * batch_var\n",
    "\n",
    "            # 使用当前 batch 的统计数据进行归一化\n",
    "            mean_to_use = batch_mean\n",
    "            var_to_use = batch_var\n",
    "        else:\n",
    "            # 推理阶段，使用运行时的均值和方差\n",
    "            mean_to_use = self.running_mean\n",
    "            var_to_use = self.running_var\n",
    "\n",
    "        # 归一化\n",
    "        # 调整 mean 和 var 的形状以匹配 x_reshaped\n",
    "        mean_to_use = mean_to_use.view(1, self.num_features)\n",
    "        var_to_use = var_to_use.view(1, self.num_features)\n",
    "        x_normalized = (x_reshaped - mean_to_use) / torch.sqrt(var_to_use + self.eps)\n",
    "\n",
    "        # 应用可学习的缩放和平移\n",
    "        # 调整 gamma 和 beta 的形状\n",
    "        gamma = self.gamma.view(1, self.num_features)\n",
    "        beta = self.beta.view(1, self.num_features)\n",
    "        out_reshaped = gamma * x_normalized + beta\n",
    "\n",
    "        # 如果输入是多维的，恢复原始形状\n",
    "        if x.dim() > 2:\n",
    "            out = out_reshaped.view(x.shape)\n",
    "        else:\n",
    "            out = out_reshaped\n",
    "\n",
    "        return out\n",
    "\n",
    "# --- 示例 ---\n",
    "# 1. Layer Normalization 示例\n",
    "print(\"--- Layer Normalization Example ---\")\n",
    "batch_size, seq_len, features = 2, 3, 4\n",
    "ln_input = torch.randn(batch_size, seq_len, features)\n",
    "\n",
    "# 手动实现 LN\n",
    "manual_ln = LayerNorm(features) # 对最后一个维度 (features) 进行归一化\n",
    "manual_ln_output = manual_ln(ln_input)\n",
    "print(\"Manual LN Input:\\n\", ln_input)\n",
    "print(\"Manual LN Output:\\n\", manual_ln_output)\n",
    "\n",
    "# PyTorch 内置 LN\n",
    "pytorch_ln = nn.LayerNorm(features)\n",
    "# 同步参数以确保结果一致 (通常不需要，这里仅为验证)\n",
    "pytorch_ln.weight.data = manual_ln.gamma.data.clone()\n",
    "pytorch_ln.bias.data = manual_ln.beta.data.clone()\n",
    "pytorch_ln_output = pytorch_ln(ln_input)\n",
    "print(\"PyTorch LN Output:\\n\", pytorch_ln_output)\n",
    "print(\"Difference (LN):\", torch.allclose(manual_ln_output, pytorch_ln_output, atol=1e-6))\n",
    "\n",
    "\n",
    "# 2. Batch Normalization 示例\n",
    "print(\"\\n--- Batch Normalization Example ---\")\n",
    "bn_features = 5\n",
    "bn_input = torch.randn(batch_size, bn_features) * 2 + 1 # 加点噪声和偏移\n",
    "\n",
    "# 手动实现 BN (训练模式)\n",
    "manual_bn = BatchNorm(bn_features)\n",
    "manual_bn.train() # 确保是训练模式\n",
    "manual_bn_output_train = manual_bn(bn_input)\n",
    "print(\"\\nManual BN Input:\\n\", bn_input)\n",
    "print(\"Manual BN Output (Train):\\n\", manual_bn_output_train)\n",
    "print(\"Manual BN Running Mean (after train):\", manual_bn.running_mean)\n",
    "print(\"Manual BN Running Var (after train):\", manual_bn.running_var)\n",
    "\n",
    "\n",
    "# PyTorch 内置 BN (训练模式)\n",
    "pytorch_bn = nn.BatchNorm1d(bn_features) # BN1d 用于 (N, C) 或 (N, C, L)\n",
    "pytorch_bn.train()\n",
    "# 同步参数和状态\n",
    "pytorch_bn.weight.data = manual_bn.gamma.data.clone()\n",
    "pytorch_bn.bias.data = manual_bn.beta.data.clone()\n",
    "pytorch_bn.running_mean.data = manual_bn.running_mean.data.clone() # 同步初始运行状态\n",
    "pytorch_bn.running_var.data = manual_bn.running_var.data.clone()\n",
    "pytorch_bn_output_train = pytorch_bn(bn_input)\n",
    "print(\"\\nPyTorch BN Output (Train):\\n\", pytorch_bn_output_train)\n",
    "print(\"PyTorch BN Running Mean (after train):\", pytorch_bn.running_mean)\n",
    "print(\"PyTorch BN Running Var (after train):\", pytorch_bn.running_var)\n",
    "print(\"Difference (BN Train):\", torch.allclose(manual_bn_output_train, pytorch_bn_output_train, atol=1e-6))\n",
    "\n",
    "\n",
    "# 手动实现 BN (评估模式)\n",
    "manual_bn.eval() # 切换到评估模式\n",
    "bn_input_eval = torch.randn(batch_size, bn_features)\n",
    "manual_bn_output_eval = manual_bn(bn_input_eval) # 使用之前计算的 running mean/var\n",
    "print(\"\\nManual BN Input (Eval):\\n\", bn_input_eval)\n",
    "print(\"Manual BN Output (Eval):\\n\", manual_bn_output_eval)\n",
    "\n",
    "# PyTorch 内置 BN (评估模式)\n",
    "pytorch_bn.eval()\n",
    "pytorch_bn_output_eval = pytorch_bn(bn_input_eval)\n",
    "print(\"PyTorch BN Output (Eval):\\n\", pytorch_bn_output_eval)\n",
    "print(\"Difference (BN Eval):\", torch.allclose(manual_bn_output_eval, pytorch_bn_output_eval, atol=1e-6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce258cd",
   "metadata": {},
   "source": [
    "# Flash Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20146dea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
